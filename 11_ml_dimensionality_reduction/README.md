# Machine learning: dimensionality reduction

By Gianluca Campanella (<g.campanella@estimand.com>)

[![Creative Commons License](https://i.creativecommons.org/l/by/4.0/80x15.png)](http://creativecommons.org/licenses/by/4.0/)

## Objectives

By the end of the session, you should be able to:

* Describe the main differences between regression and dimensionality reduction
* Define the outputs of a Principal Component Analysis
* Use `scikit-learn` to perform PCA and PLS

## Plan

The session is designed to be delivered over three hours (including breaks).

| Topic                                         | Time        |
| --------------------------------------------- | ----------- |
| Introduction to dimensionality reduction      | 60 minutes  |
| Dimensionality reduction using `scikit-learn` | 90 minutes  |

## Materials

* [Introduction to dimensionality reduction](https://cdn.rawgit.com/estimand/ga-data-science/master/11_ml_dimensionality_reduction/slides/intro_dimensionality_reduction.pdf)
* [Dimensionality reduction using `scikit-learn`](https://cdn.rawgit.com/estimand/ga-data-science/master/11_ml_dimensionality_reduction/notebooks/01_dimensionality_reduction.ipynb)

## Additional resources

* [Principal Component Analysis Explained Visually](http://setosa.io/ev/principal-component-analysis/)
* [Everything you did and didn't know about PCA](http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/)

